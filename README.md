# 2. CNN using Iris Features (Image-style Classification)
## âœ… Improved Code â€“ Key Points

Converted Iris numerical features into 2Ã—2 image format

Applied feature normalization for stable CNN training

Used a lightweight CNN architecture suitable for small data

Added clear prediction output with flower class name

Displayed the feature-image with predicted class for visibility

## ðŸ“˜ Key Points

Iris dataset does not contain images; hence features were reshaped into image form

CNN was used to learn spatial relationships among features

Model classifies Iris flowers into Setosa, Versicolor, and Virginica

Output clearly displays predicted flower category

Approach demonstrates innovative use of CNN on tabular data

# 3. Q-Learning Path Finding on Graph (Police & Drug Traces)
## âœ… Improved Code â€“ Key Points

Fixed action selection bug in Q-learning exploration

Simplified reward matrix initialization

Removed duplicate functions and cleaned logic

Added environment-aware learning (police & drug traces)

Improved visualization of rewards and learned paths

## ðŸ“˜ Key Points

Implemented Q-learning to find optimal path in a graph

Goal node rewarded to guide learning

Environment factors like police and drug traces were incorporated

Agent learns both efficiency and safety

Demonstrates reinforcement learning in real-world navigation scenarios

# 4. LSTM Time-Series Forecasting (Airline Passengers)
## âœ… Improved Code â€“ Key Points

Removed hard-coded dataset path for portability

Added reusable time-series dataset creation function

Clearly reshaped data for LSTM input requirements

Improved visualization of predictions vs actual data

Used RMSE for proper model evaluation

## ðŸ“˜ Key Points

LSTM model predicts airline passenger traffic over time

Data normalized using Min-Max scaling

Time-step window captures temporal dependency

Model performance evaluated using RMSE

Results show effective learning of sequential patterns

# 5. Character-Level RNN Text Generation
## âœ… Improved Code â€“ Key Points

Clarified RNN hidden unit configuration

Removed redundant commented code

Simplified text generation logic

Added meaningful variable names and structure

Ensured consistent sequence length during prediction

## ðŸ“˜ Key Points

Character-level RNN trained on given text input

Model learns character sequence dependencies

Uses one-hot encoding for character representation

Generates new text based on learned patterns

Demonstrates sequence learning using SimpleRNN
